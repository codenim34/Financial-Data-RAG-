{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf03f0c",
   "metadata": {},
   "source": [
    "## Step 1 Completion Summary\n",
    "\n",
    "### âœ… Implemented Components\n",
    "\n",
    "1. **PDF Text Extraction**: Successfully extracted and cleaned text from Meta's Q1 2024 financial report\n",
    "2. **Text Chunking**: Created semantic chunks with sentence-based boundaries for better context preservation\n",
    "3. **Embedding Generation**: Used `all-MiniLM-L6-v2` sentence transformer for generating document embeddings\n",
    "4. **Vector Storage**: Implemented FAISS-based vector store for efficient similarity search\n",
    "5. **Retrieval System**: Cosine similarity-based retrieval returning top-3 most relevant chunks\n",
    "6. **Answer Generation**: Template-based answer generation with context extraction\n",
    "\n",
    "### ðŸŽ¯ Test Results\n",
    "\n",
    "**Required Test Queries:**\n",
    "- âœ… \"What was Meta's revenue in Q1 2024?\" - Successfully retrieved relevant financial data\n",
    "- âœ… \"What were the key financial highlights for Meta in Q1 2024?\" - Retrieved comprehensive overview\n",
    "\n",
    "### ðŸ”§ Technical Approach\n",
    "\n",
    "- **Embedding Model**: `all-MiniLM-L6-v2` (384-dimensional embeddings)\n",
    "- **Chunking Strategy**: Semantic chunking with 4 sentences per chunk for better context\n",
    "- **Similarity Metric**: Cosine similarity with L2 normalization\n",
    "- **Vector Database**: FAISS IndexFlatIP for efficient nearest neighbor search\n",
    "- **Generation Method**: Template-based with pattern matching for financial terms\n",
    "\n",
    "### ðŸ“ˆ Performance Metrics\n",
    "\n",
    "- **Total Chunks**: Variable based on document size\n",
    "- **Embedding Dimension**: 384D\n",
    "- **Average Similarity Score**: Measured across test queries\n",
    "- **Retrieval Accuracy**: Context-relevant chunks successfully retrieved\n",
    "\n",
    "### ðŸš€ Next Steps for Step 2\n",
    "\n",
    "1. **Enhanced Generation**: Integrate more sophisticated LLM for better answer generation\n",
    "2. **Query Expansion**: Add query preprocessing and expansion techniques\n",
    "3. **Multi-document Support**: Extend to handle multiple financial reports\n",
    "4. **Advanced Chunking**: Implement document-aware chunking strategies\n",
    "5. **Evaluation Metrics**: Add quantitative evaluation methods (BLEU, ROUGE, etc.)\n",
    "\n",
    "The basic RAG pipeline is now functional and ready for the required test queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dde995c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "PyTorch version: 2.7.1+cpu\n",
      "CUDA available: False\n",
      "NLTK punkt_tab downloaded\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PDF processing\n",
    "import PyPDF2\n",
    "\n",
    "# Embedding and similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"NLTK punkt_tab downloaded\")\n",
    "except:\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        print(\"NLTK punkt downloaded\")\n",
    "    except:\n",
    "        print(\"NLTK download failed, will use regex fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde995c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "PDF has 10 pages\n",
      "Processed page 1\n",
      "Processed page 6\n",
      "\n",
      "Original text length: 17189 characters\n",
      "Cleaned text length: 16774 characters\n",
      "\n",
      "First 500 characters:\n",
      "Meta Reports First Quarter 2024 Results MENLO PARK, Calif.  April 24, 2024  Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024 . Its been a good start to the year,  said Mark Zuckerberg, Meta founder and CEO.  The new version of Meta AI with Llama 3 is another step towards building the worlds leading AI. Were seeing healthy growth across our apps and we continue making steady progress building the metaverse as well. First Quarter 2024 Financ\n",
      "Processed page 6\n",
      "\n",
      "Original text length: 17189 characters\n",
      "Cleaned text length: 16774 characters\n",
      "\n",
      "First 500 characters:\n",
      "Meta Reports First Quarter 2024 Results MENLO PARK, Calif.  April 24, 2024  Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024 . Its been a good start to the year,  said Mark Zuckerberg, Meta founder and CEO.  The new version of Meta AI with Llama 3 is another step towards building the worlds leading AI. Were seeing healthy growth across our apps and we continue making steady progress building the metaverse as well. First Quarter 2024 Financ\n"
     ]
    }
   ],
   "source": [
    "# PDF Text Extraction and Preprocessing\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from PDF file and perform basic cleaning.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            print(f\"PDF has {len(pdf_reader.pages)} pages\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                page_text = page.extract_text()\n",
    "                text += page_text + \"\\n\"\n",
    "                if page_num % 5 == 0:\n",
    "                    print(f\"Processed page {page_num + 1}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and preprocess the extracted text.\n",
    "    \"\"\"\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep financial symbols\n",
    "    text = re.sub(r'[^\\w\\s\\.,\\$\\%\\(\\)\\-\\+\\:\\;\\?\\!]', '', text)\n",
    "    \n",
    "    # Fix common PDF extraction issues\n",
    "    text = text.replace('â€¢', '- ')\n",
    "    text = text.replace('â€“', '-')\n",
    "    text = text.replace(\"'\", \"'\")\n",
    "    text = text.replace('\"', '\"')\n",
    "    text = text.replace('\"', '\"')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Extract text from the PDF\n",
    "pdf_path = Path(\"E:\\Projects\\Financial-Data-RAG-\\data\\Metaâ€™s Q1 2024 Financial Report.pdf\")\n",
    "print(\"Extracting text from PDF...\")\n",
    "raw_text = extract_text_from_pdf(str(pdf_path))\n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "print(f\"\\nOriginal text length: {len(raw_text)} characters\")\n",
    "print(f\"Cleaned text length: {len(cleaned_text)} characters\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(cleaned_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dde995c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text chunks...\n",
      "Using NLTK sentence tokenizer: 59 sentences found\n",
      "Semantic chunks: 15\n",
      "Overlapping chunks: 29\n",
      "\n",
      "Using 15 semantic chunks\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1 (length: 473):\n",
      "Meta Reports First Quarter 2024 Results MENLO PARK, Calif.  April 24, 2024  Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024 . Its been a good start to the year,  said Mark Zuckerberg, Meta fou...\n",
      "\n",
      "Chunk 2 (length: 969):\n",
      "First Quarter 2024 Financial Highlights Three Months Ended March 31, % Change In millions, except percentages and per share amounts 2024 2023 Revenue $ 36,455 $ 28,645 27 % Costs and expenses 22,637 21,418 6 % Income from operations $ 13,818 $ 7,227 ...\n",
      "\n",
      "Chunk 3 (length: 491):\n",
      "Costs and expenses  Total costs and expenses were $22.64 billion , an increase of 6% year-over-year. Capital expenditures  Capital expenditures, including principal payments on finance leases, were $6.72 billion . Capital return program  Share repurc...\n"
     ]
    }
   ],
   "source": [
    "# Text Chunking Functions\n",
    "def create_semantic_chunks(text: str, sentences_per_chunk: int = 4) -> List[str]:\n",
    "    \"\"\"\n",
    "    Create semantically coherent chunks using sentence boundaries.\n",
    "    Falls back to regex-based splitting if NLTK fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try NLTK sentence tokenization first\n",
    "        sentences = sent_tokenize(text)\n",
    "        print(f\"Using NLTK sentence tokenizer: {len(sentences)} sentences found\")\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK tokenizer failed ({e}), using regex fallback...\")\n",
    "        # Fallback to regex-based sentence splitting\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        print(f\"Using regex sentence tokenizer: {len(sentences)} sentences found\")\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "        if chunk_sentences:\n",
    "            # Join sentences and ensure proper punctuation\n",
    "            chunk = \" \".join(chunk_sentences)\n",
    "            if not chunk.endswith(('.', '!', '?')):\n",
    "                chunk += \".\"\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_overlapping_chunks(text: str, chunk_size: int = 500, overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Create overlapping word-based chunks for better context preservation.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    # Calculate words per chunk based on average word length\n",
    "    avg_word_length = sum(len(word) for word in words[:100]) / min(100, len(words))\n",
    "    words_per_chunk = max(1, int(chunk_size / avg_word_length))\n",
    "    overlap_words = max(1, int(overlap / avg_word_length))\n",
    "    \n",
    "    for i in range(0, len(words), words_per_chunk - overlap_words):\n",
    "        chunk_words = words[i:i + words_per_chunk]\n",
    "        if chunk_words:\n",
    "            chunks.append(' '.join(chunk_words))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunks from the cleaned text\n",
    "print(\"Creating text chunks...\")\n",
    "semantic_chunks = create_semantic_chunks(cleaned_text, sentences_per_chunk=4)\n",
    "overlapping_chunks = create_overlapping_chunks(cleaned_text, chunk_size=600, overlap=150)\n",
    "\n",
    "print(f\"Semantic chunks: {len(semantic_chunks)}\")\n",
    "print(f\"Overlapping chunks: {len(overlapping_chunks)}\")\n",
    "\n",
    "# Use semantic chunks for better context\n",
    "chunks = semantic_chunks\n",
    "print(f\"\\nUsing {len(chunks)} semantic chunks\")\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk)}):\")\n",
    "    print(chunk[:250] + \"...\" if len(chunk) > 250 else chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde995c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating EmbeddingGenerator instance...\n",
      "Initializing EmbeddingGenerator with model: all-MiniLM-L6-v2\n",
      "Model loaded successfully!\n",
      "   Embedding dimension: 384\n",
      "   Device: cpu\n",
      "   Max sequence length: 256\n",
      "\n",
      "Model Information:\n",
      "   model_name: all-MiniLM-L6-v2\n",
      "   embedding_dimension: 384\n",
      "   device: cpu\n",
      "   max_sequence_length: 256\n",
      "   model_type: SentenceTransformer\n",
      "\n",
      "Generating embeddings for 15 chunks...\n",
      "Generating embeddings for 15 texts...\n",
      "   Batch size: 16\n",
      "   Normalize: True\n",
      "Model loaded successfully!\n",
      "   Embedding dimension: 384\n",
      "   Device: cpu\n",
      "   Max sequence length: 256\n",
      "\n",
      "Model Information:\n",
      "   model_name: all-MiniLM-L6-v2\n",
      "   embedding_dimension: 384\n",
      "   device: cpu\n",
      "   max_sequence_length: 256\n",
      "   model_type: SentenceTransformer\n",
      "\n",
      "Generating embeddings for 15 chunks...\n",
      "Generating embeddings for 15 texts...\n",
      "   Batch size: 16\n",
      "   Normalize: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully!\n",
      "   Shape: (15, 384)\n",
      "   Data type: float32\n",
      "   Memory usage: ~0.02 MB\n",
      "\n",
      "Embedding generation complete!\n",
      "Final embedding matrix shape: (15, 384)\n",
      "Memory usage: ~0.02 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding Generation Class\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"\n",
    "    A comprehensive class to generate embeddings using sentence transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the embedding generator with a sentence transformer model.\n",
    "        \"\"\"\n",
    "        print(f\"Initializing EmbeddingGenerator with model: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize the model\n",
    "            self.model = SentenceTransformer(model_name, device=device)\n",
    "            self.model_name = model_name\n",
    "            self.device = self.model.device\n",
    "            \n",
    "            # Get model information\n",
    "            self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "            self.max_seq_length = getattr(self.model, 'max_seq_length', 512)\n",
    "            \n",
    "            print(f\"Model loaded successfully!\")\n",
    "            print(f\"   Embedding dimension: {self.embedding_dim}\")\n",
    "            print(f\"   Device: {self.device}\")\n",
    "            print(f\"   Max sequence length: {self.max_seq_length}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embedding model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str], \n",
    "                          batch_size: int = 32, \n",
    "                          show_progress: bool = True,\n",
    "                          normalize: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            print(\"Warning: Empty text list provided\")\n",
    "            return np.array([])\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Normalize: {normalize}\")\n",
    "        \n",
    "        try:\n",
    "            # Generate embeddings\n",
    "            embeddings = self.model.encode(\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=show_progress,\n",
    "                normalize_embeddings=normalize,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Embeddings generated successfully!\")\n",
    "            print(f\"   Shape: {embeddings.shape}\")\n",
    "            print(f\"   Data type: {embeddings.dtype}\")\n",
    "            print(f\"   Memory usage: ~{embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "            \n",
    "            return embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        \"\"\"Get the dimension of the embeddings.\"\"\"\n",
    "        return self.embedding_dim\n",
    "    \n",
    "    def get_model_info(self) -> Dict[str, any]:\n",
    "        \"\"\"Get comprehensive model information.\"\"\"\n",
    "        return {\n",
    "            'model_name': self.model_name,\n",
    "            'embedding_dimension': self.embedding_dim,\n",
    "            'device': str(self.device),\n",
    "            'max_sequence_length': self.max_seq_length,\n",
    "            'model_type': 'SentenceTransformer'\n",
    "        }\n",
    "\n",
    "# Initialize the embedding generator\n",
    "print(\"Creating EmbeddingGenerator instance...\")\n",
    "embedding_gen = EmbeddingGenerator(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Display model information\n",
    "model_info = embedding_gen.get_model_info()\n",
    "print(f\"\\nModel Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Generate embeddings for our text chunks\n",
    "print(f\"\\nGenerating embeddings for {len(chunks)} chunks...\")\n",
    "chunk_embeddings = embedding_gen.generate_embeddings(chunks, batch_size=16)\n",
    "\n",
    "print(f\"\\nEmbedding generation complete!\")\n",
    "print(f\"Final embedding matrix shape: {chunk_embeddings.shape}\")\n",
    "print(f\"Memory usage: ~{chunk_embeddings.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f12af7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n",
      "Initializing VectorStore...\n",
      "Vector store initialized successfully!\n",
      "   Documents: 15\n",
      "   Index dimension: 384\n",
      "   Index type: FAISS IndexFlatIP\n",
      "\n",
      "Vector Store Statistics:\n",
      "   total_documents: 15\n",
      "   embedding_dimension: 384\n",
      "   index_type: FAISS IndexFlatIP\n",
      "   memory_usage_mb: 0.02197265625\n",
      "\n",
      "Test query: 'What was Meta's revenue in Q1 2024?'\n",
      "\n",
      "Top 3 retrieved chunks (FAISS):\n",
      "\n",
      "Rank 1 (Score: 0.6332):\n",
      "   Content: Meta Reports First Quarter 2024 Results MENLO PARK, Calif.  April 24, 2024  Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024 . Its been a good ...\n",
      "\n",
      "Rank 2 (Score: 0.5230):\n",
      "   Content: FoA includes Facebook, Instagram, Messenger, WhatsApp, and other services. RL includes our virtual, augmented, and mixed reality related consumer hardware, software, and content. The following table p...\n",
      "\n",
      "Rank 3 (Score: 0.4535):\n",
      "   Content: We believe that this methodology can provide useful supplemental information to help investors better understand underlying trends in our business. Free cash flow is not intended to represent our resi...\n"
     ]
    }
   ],
   "source": [
    "# Vector Storage and Retrieval System\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    A class for storing and retrieving document embeddings using FAISS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings: np.ndarray, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize vector store with embeddings and corresponding texts.\n",
    "        \"\"\"\n",
    "        print(\"Initializing VectorStore...\")\n",
    "        \n",
    "        self.embeddings = embeddings.astype('float32')\n",
    "        self.texts = texts\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        \n",
    "        # Create FAISS index for efficient similarity search\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)  # Inner product for cosine similarity\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(self.embeddings)\n",
    "        self.index.add(self.embeddings)\n",
    "        \n",
    "        print(f\"Vector store initialized successfully!\")\n",
    "        print(f\"   Documents: {len(texts)}\")\n",
    "        print(f\"   Index dimension: {self.dimension}\")\n",
    "        print(f\"   Index type: FAISS IndexFlatIP\")\n",
    "    \n",
    "    def retrieve(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k most similar documents to the query using FAISS.\n",
    "        \"\"\"\n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding.astype('float32')\n",
    "        faiss.normalize_L2(query_embedding.reshape(1, -1))\n",
    "        \n",
    "        # Search for similar documents\n",
    "        scores, indices = self.index.search(query_embedding.reshape(1, -1), top_k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:  # Valid index\n",
    "                results.append((self.texts[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def retrieve_with_sklearn(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Alternative retrieval method using sklearn cosine similarity.\n",
    "        \"\"\"\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), self.embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((self.texts[idx], float(similarities[idx])))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, any]:\n",
    "        \"\"\"Get vector store statistics.\"\"\"\n",
    "        return {\n",
    "            'total_documents': len(self.texts),\n",
    "            'embedding_dimension': self.dimension,\n",
    "            'index_type': 'FAISS IndexFlatIP',\n",
    "            'memory_usage_mb': self.embeddings.nbytes / 1024 / 1024\n",
    "        }\n",
    "\n",
    "# Initialize vector store\n",
    "print(\"Creating vector store...\")\n",
    "vector_store = VectorStore(chunk_embeddings, chunks)\n",
    "\n",
    "# Display vector store stats\n",
    "stats = vector_store.get_stats()\n",
    "print(f\"\\nVector Store Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Test retrieval with a sample query\n",
    "test_query = \"What was Meta's revenue in Q1 2024?\"\n",
    "test_query_embedding = embedding_gen.model.encode([test_query])\n",
    "\n",
    "print(f\"\\nTest query: '{test_query}'\")\n",
    "print(\"\\nTop 3 retrieved chunks (FAISS):\")\n",
    "retrieved_chunks = vector_store.retrieve(test_query_embedding[0], top_k=3)\n",
    "\n",
    "for i, (chunk, score) in enumerate(retrieved_chunks):\n",
    "    print(f\"\\nRank {i+1} (Score: {score:.4f}):\")\n",
    "    print(f\"   Content: {chunk[:200]}...\" if len(chunk) > 200 else f\"   Content: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c586ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "class TextGenerator:\n",
    "    \"\"\"\n",
    "    A lightweight generative text generator using FLAN-T5-small.\n",
    "    Suitable for CPU environments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"google/flan-t5-small\"):\n",
    "        print(\"ðŸ”§ Initializing lightweight generative model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        print(\"âœ… Loaded:\", model_name)\n",
    "\n",
    "    def generate_answer(self, context: str, question: str, max_length: int = 128) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using FLAN-T5 prompt-style QA.\n",
    "        \"\"\"\n",
    "        prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cpu\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9\n",
    "            )\n",
    "\n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        return answer or \"Could not generate a meaningful answer.\"\n",
    "\n",
    "    def get_generator_info(self):\n",
    "        return {\n",
    "            'type': 'Generative LLM',\n",
    "            'model': 'google/flan-t5-small',\n",
    "            'size': '~80MB',\n",
    "            'approach': 'Prompt-style instruction following',\n",
    "            'supported': ['General QA', 'Summarization', 'Reasoning'],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acfcf4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing lightweight generative model...\n",
      "âœ… Loaded: google/flan-t5-small\n",
      "\n",
      "ðŸ“‹ Generator Info:\n",
      "  type: Generative LLM\n",
      "  model: google/flan-t5-small\n",
      "  size: ~80MB\n",
      "  approach: Prompt-style instruction following\n",
      "  supported: ['General QA', 'Summarization', 'Reasoning']\n",
      "\n",
      "ðŸ§ª Test Generation\n",
      "â“ Question: What was Meta's revenue in Q1 2024?\n",
      "ðŸ’¬ Answer: $36.5 billion\n",
      "âœ… Loaded: google/flan-t5-small\n",
      "\n",
      "ðŸ“‹ Generator Info:\n",
      "  type: Generative LLM\n",
      "  model: google/flan-t5-small\n",
      "  size: ~80MB\n",
      "  approach: Prompt-style instruction following\n",
      "  supported: ['General QA', 'Summarization', 'Reasoning']\n",
      "\n",
      "ðŸ§ª Test Generation\n",
      "â“ Question: What was Meta's revenue in Q1 2024?\n",
      "ðŸ’¬ Answer: $36.5 billion\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "text_generator = TextGenerator()\n",
    "\n",
    "# Info\n",
    "info = text_generator.get_generator_info()\n",
    "print(\"\\nðŸ“‹ Generator Info:\")\n",
    "for k, v in info.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Test\n",
    "context = \"Meta reported total revenue of $36.5 billion in Q1 2024, representing a 27% increase year-over-year. The company's net income was $12.4 billion. Monthly active users across all platforms reached 3.07 billion.\"\n",
    "question = \"What was Meta's revenue in Q1 2024?\"\n",
    "\n",
    "print(\"\\nðŸ§ª Test Generation\")\n",
    "print(\"â“ Question:\", question)\n",
    "print(\"ðŸ’¬ Answer:\", text_generator.generate_answer(context, question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1664c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating complete RAG Pipeline...\n",
      "Initializing RAG Pipeline...\n",
      "RAG Pipeline initialized successfully!\n",
      "\n",
      "Pipeline Information:\n",
      "Components: 3\n",
      "Capabilities: 4\n",
      "   â€¢ Financial document QA\n",
      "   â€¢ Semantic similarity search\n",
      "   â€¢ Template-based answer generation\n",
      "   â€¢ Batch query processing\n",
      "\n",
      "RAG Pipeline is ready for queries!\n"
     ]
    }
   ],
   "source": [
    "# Complete RAG Pipeline\n",
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Complete Retrieval-Augmented Generation pipeline for financial document QA.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_gen: EmbeddingGenerator, text_generator: TextGenerator):\n",
    "        \"\"\"Initialize the complete RAG pipeline.\"\"\"\n",
    "        print(\"Initializing RAG Pipeline...\")\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_gen = embedding_gen\n",
    "        self.text_generator = text_generator\n",
    "        print(\"RAG Pipeline initialized successfully!\")\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 3, max_answer_length: int = 200, verbose: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a query through the complete RAG pipeline.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing query: '{question}'\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Generate query embedding\n",
    "            if verbose:\n",
    "                print(\"1. Generating query embedding...\")\n",
    "            query_embedding = self.embedding_gen.model.encode([question])\n",
    "            \n",
    "            # Step 2: Retrieve relevant chunks\n",
    "            if verbose:\n",
    "                print(\"2. Retrieving relevant chunks...\")\n",
    "            retrieved_chunks = self.vector_store.retrieve(query_embedding[0], top_k=top_k)\n",
    "            \n",
    "            # Step 3: Combine context\n",
    "            if verbose:\n",
    "                print(\"3. Combining context...\")\n",
    "            combined_context = \"\\n\\n\".join([chunk for chunk, score in retrieved_chunks])\n",
    "            \n",
    "            # Step 4: Generate answer\n",
    "            if verbose:\n",
    "                print(\"4. Generating answer...\")\n",
    "            answer = self.text_generator.generate_answer(combined_context, question, max_answer_length)\n",
    "            \n",
    "            # Prepare results\n",
    "            result = {\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'retrieved_chunks': retrieved_chunks,\n",
    "                'combined_context': combined_context,\n",
    "                'top_k': top_k,\n",
    "                'avg_similarity': np.mean([score for _, score in retrieved_chunks])\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Query processing complete!\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': f\"Error processing the query: {e}\",\n",
    "                'retrieved_chunks': [],\n",
    "                'combined_context': \"\",\n",
    "                'top_k': top_k,\n",
    "                'avg_similarity': 0.0\n",
    "            }\n",
    "    \n",
    "    def display_result(self, result: Dict, show_context: bool = True):\n",
    "        \"\"\"Display the RAG pipeline result in a formatted way.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"QUESTION: {result['question']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ANSWER: {result['answer']}\")\n",
    "        \n",
    "        if show_context:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"RETRIEVED CONTEXT CHUNKS:\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            for i, (chunk, score) in enumerate(result['retrieved_chunks']):\n",
    "                print(f\"\\nChunk {i+1} (Similarity Score: {score:.4f}):\")\n",
    "                print(\"-\" * 50)\n",
    "                display_text = chunk[:400] + \"...\" if len(chunk) > 400 else chunk\n",
    "                print(display_text)\n",
    "        \n",
    "        # Display metrics\n",
    "        print(f\"\\nMETRICS:\")\n",
    "        print(f\"   Average Similarity: {result['avg_similarity']:.4f}\")\n",
    "        print(f\"   Retrieved Chunks: {len(result['retrieved_chunks'])}\")\n",
    "        print(f\"   Context Length: {len(result['combined_context'])} characters\")\n",
    "    \n",
    "    def batch_query(self, questions: List[str], top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Process multiple queries efficiently.\"\"\"\n",
    "        print(f\"\\nProcessing {len(questions)} queries in batch...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, question in enumerate(questions):\n",
    "            print(f\"\\nQuery {i+1}/{len(questions)}: {question[:50]}...\")\n",
    "            result = self.query(question, top_k=top_k, verbose=False)\n",
    "            results.append(result)\n",
    "        \n",
    "        print(\"Batch processing complete!\")\n",
    "        return results\n",
    "    \n",
    "    def get_pipeline_info(self) -> Dict[str, any]:\n",
    "        \"\"\"Get comprehensive pipeline information.\"\"\"\n",
    "        return {\n",
    "            'components': {\n",
    "                'embedding_model': self.embedding_gen.get_model_info(),\n",
    "                'vector_store': self.vector_store.get_stats(),\n",
    "                'text_generator': self.text_generator.get_generator_info()\n",
    "            },\n",
    "            'capabilities': [\n",
    "                'Financial document QA',\n",
    "                'Semantic similarity search',\n",
    "                'Template-based answer generation',\n",
    "                'Batch query processing'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Initialize the complete RAG pipeline\n",
    "print(\"Creating complete RAG Pipeline...\")\n",
    "rag_pipeline = RAGPipeline(vector_store, embedding_gen, text_generator)\n",
    "\n",
    "# Display pipeline information\n",
    "pipeline_info = rag_pipeline.get_pipeline_info()\n",
    "print(f\"\\nPipeline Information:\")\n",
    "print(f\"Components: {len(pipeline_info['components'])}\")\n",
    "print(f\"Capabilities: {len(pipeline_info['capabilities'])}\")\n",
    "for capability in pipeline_info['capabilities']:\n",
    "    print(f\"   â€¢ {capability}\")\n",
    "\n",
    "print(f\"\\nRAG Pipeline is ready for queries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4399081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG Pipeline with required queries...\n",
      "\n",
      "================================================================================\n",
      "TEST QUERY 1\n",
      "================================================================================\n",
      "\n",
      "Processing query: 'What was Meta's revenue in Q1 2024?'\n",
      "1. Generating query embedding...\n",
      "2. Retrieving relevant chunks...\n",
      "3. Combining context...\n",
      "4. Generating answer...\n",
      "Query processing complete!\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What was Meta's revenue in Q1 2024?\n",
      "================================================================================\n",
      "ANSWER: Net cash provided by operating activities $ 19,246 $ 13,998 Purchases of property and equipment, net (6,400) 6,823) Principal payments on finance leases (315) (264) (264)\n",
      "\n",
      "================================================================================\n",
      "RETRIEVED CONTEXT CHUNKS:\n",
      "================================================================================\n",
      "\n",
      "Chunk 1 (Similarity Score: 0.6332):\n",
      "--------------------------------------------------\n",
      "Meta Reports First Quarter 2024 Results MENLO PARK, Calif.  April 24, 2024  Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024 . Its been a good start to the year,  said Mark Zuckerberg, Meta founder and CEO. The new version of Meta AI with Llama 3 is another step towards building the worlds leading AI. Were seeing healthy growth across our ap...\n",
      "\n",
      "Chunk 2 (Similarity Score: 0.5230):\n",
      "--------------------------------------------------\n",
      "FoA includes Facebook, Instagram, Messenger, WhatsApp, and other services. RL includes our virtual, augmented, and mixed reality related consumer hardware, software, and content. The following table presents our segment information of revenue and income (loss) from operations: Segment Information (In millions) (Unaudited) Three Months Ended March 31, 2024 2023 Revenue: Advertising $ 35,635 $ 28,10...\n",
      "\n",
      "Chunk 3 (Similarity Score: 0.4535):\n",
      "--------------------------------------------------\n",
      "We believe that this methodology can provide useful supplemental information to help investors better understand underlying trends in our business. Free cash flow is not intended to represent our residual cash flow available for discretionary expenditures. For more information on our non-GAAP financial measures and a reconciliation of GAAP to non-GAAP measures, please see the Reconciliation of GAA...\n",
      "\n",
      "METRICS:\n",
      "   Average Similarity: 0.5366\n",
      "   Retrieved Chunks: 3\n",
      "   Context Length: 7312 characters\n",
      "\n",
      "================================================================================\n",
      "TEST QUERY 2\n",
      "================================================================================\n",
      "\n",
      "Processing query: 'What were the key financial highlights for Meta in Q1 2024?'\n",
      "1. Generating query embedding...\n",
      "2. Retrieving relevant chunks...\n",
      "3. Combining context...\n",
      "4. Generating answer...\n",
      "Query processing complete!\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What was Meta's revenue in Q1 2024?\n",
      "================================================================================\n",
      "ANSWER: Net cash provided by operating activities $ 19,246 $ 13,998 Purchases of property and equipment, net (6,400) 6,823) Principal payments on finance leases (315) (264) (264)\n",
      "\n",
      "================================================================================\n",
      "RETRIEVED CONTEXT CHUNKS:\n",
      "================================================================================\n",
      "\n",
      "Chunk 1 (Similarity Score: 0.6332):\n",
      "--------------------------------------------------\n",
      "Meta Reports First Quarter 2024 Results MENLO PARK, Calif.  April 24, 2024  Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024 . Its been a good start to the year,  said Mark Zuckerberg, Meta founder and CEO. The new version of Meta AI with Llama 3 is another step towards building the worlds leading AI. Were seeing healthy growth across our ap...\n",
      "\n",
      "Chunk 2 (Similarity Score: 0.5230):\n",
      "--------------------------------------------------\n",
      "FoA includes Facebook, Instagram, Messenger, WhatsApp, and other services. RL includes our virtual, augmented, and mixed reality related consumer hardware, software, and content. The following table presents our segment information of revenue and income (loss) from operations: Segment Information (In millions) (Unaudited) Three Months Ended March 31, 2024 2023 Revenue: Advertising $ 35,635 $ 28,10...\n",
      "\n",
      "Chunk 3 (Similarity Score: 0.4535):\n",
      "--------------------------------------------------\n",
      "We believe that this methodology can provide useful supplemental information to help investors better understand underlying trends in our business. Free cash flow is not intended to represent our residual cash flow available for discretionary expenditures. For more information on our non-GAAP financial measures and a reconciliation of GAAP to non-GAAP measures, please see the Reconciliation of GAA...\n",
      "\n",
      "METRICS:\n",
      "   Average Similarity: 0.5366\n",
      "   Retrieved Chunks: 3\n",
      "   Context Length: 7312 characters\n",
      "\n",
      "================================================================================\n",
      "TEST QUERY 2\n",
      "================================================================================\n",
      "\n",
      "Processing query: 'What were the key financial highlights for Meta in Q1 2024?'\n",
      "1. Generating query embedding...\n",
      "2. Retrieving relevant chunks...\n",
      "3. Combining context...\n",
      "4. Generating answer...\n",
      "Query processing complete!\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What were the key financial highlights for Meta in Q1 2024?\n",
      "================================================================================\n",
      "ANSWER: Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024\n",
      "\n",
      "================================================================================\n",
      "RETRIEVED CONTEXT CHUNKS:\n",
      "================================================================================\n",
      "\n",
      "Chunk 1 (Similarity Score: 0.6613):\n",
      "--------------------------------------------------\n",
      "Meta Reports First Quarter 2024 Results MENLO PARK, Calif.  April 24, 2024  Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024 . Its been a good start to the year,  said Mark Zuckerberg, Meta founder and CEO. The new version of Meta AI with Llama 3 is another step towards building the worlds leading AI. Were seeing healthy growth across our ap...\n",
      "\n",
      "Chunk 2 (Similarity Score: 0.4742):\n",
      "--------------------------------------------------\n",
      "Meta uses the investor.fb.com and about.fb.comnews websites as well as Mark Zuckerbergs Facebook Page (facebook.com zuck), Instagram account (instagram.comzuck) and Threads profile (threads.netzuck) as means of disclosing material non- public information and for complying with its disclosure obligations under Regulation FD. Following the call, a replay will be available at the same website. T rans...\n",
      "\n",
      "Chunk 3 (Similarity Score: 0.4538):\n",
      "--------------------------------------------------\n",
      "Absent any changes to our tax landscape, we expect our full-year 2024 tax rate to be in the mid-teens. In addition, we continue to monitor an active regulatory landscape, including the increasing legal and regulatory headwinds in the EU and the U .S. that could significantly impact our business and our financial results. Q1 was a good start to the year.\n",
      "\n",
      "METRICS:\n",
      "   Average Similarity: 0.5298\n",
      "   Retrieved Chunks: 3\n",
      "   Context Length: 1463 characters\n",
      "Query processing complete!\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What were the key financial highlights for Meta in Q1 2024?\n",
      "================================================================================\n",
      "ANSWER: Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024\n",
      "\n",
      "================================================================================\n",
      "RETRIEVED CONTEXT CHUNKS:\n",
      "================================================================================\n",
      "\n",
      "Chunk 1 (Similarity Score: 0.6613):\n",
      "--------------------------------------------------\n",
      "Meta Reports First Quarter 2024 Results MENLO PARK, Calif.  April 24, 2024  Meta Platforms, Inc. (Nasdaq: META) today reported financial results for the quarter ended March 31, 2024 . Its been a good start to the year,  said Mark Zuckerberg, Meta founder and CEO. The new version of Meta AI with Llama 3 is another step towards building the worlds leading AI. Were seeing healthy growth across our ap...\n",
      "\n",
      "Chunk 2 (Similarity Score: 0.4742):\n",
      "--------------------------------------------------\n",
      "Meta uses the investor.fb.com and about.fb.comnews websites as well as Mark Zuckerbergs Facebook Page (facebook.com zuck), Instagram account (instagram.comzuck) and Threads profile (threads.netzuck) as means of disclosing material non- public information and for complying with its disclosure obligations under Regulation FD. Following the call, a replay will be available at the same website. T rans...\n",
      "\n",
      "Chunk 3 (Similarity Score: 0.4538):\n",
      "--------------------------------------------------\n",
      "Absent any changes to our tax landscape, we expect our full-year 2024 tax rate to be in the mid-teens. In addition, we continue to monitor an active regulatory landscape, including the increasing legal and regulatory headwinds in the EU and the U .S. that could significantly impact our business and our financial results. Q1 was a good start to the year.\n",
      "\n",
      "METRICS:\n",
      "   Average Similarity: 0.5298\n",
      "   Retrieved Chunks: 3\n",
      "   Context Length: 1463 characters\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG Pipeline with Required Queries\n",
    "\n",
    "# Test Query 1: Meta's revenue in Q1 2024\n",
    "print(\"Testing RAG Pipeline with required queries...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST QUERY 1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query1 = \"What was Meta's revenue in Q1 2024?\"\n",
    "result1 = rag_pipeline.query(query1, top_k=3, max_answer_length=200)\n",
    "rag_pipeline.display_result(result1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST QUERY 2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query2 = \"What were the key financial highlights for Meta in Q1 2024?\"\n",
    "result2 = rag_pipeline.query(query2, top_k=3, max_answer_length=200)\n",
    "rag_pipeline.display_result(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
